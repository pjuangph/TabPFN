"""Check that preprocessing pipeline output matches reference transformations.

This ensures that the preprocessing behavior does not change unintentionally
during refactoring or other changes.

The transformed outputs can vary slightly between platforms, thus we maintain
separate reference data for all the platforms in `ENABLED_PLATFORMS`. The tests
are skipped on other platforms.

If the outputs change legitimately, the reference data can be regenerated by
running this file: `python tests/test_preprocessing/test_pipeline_consistency.py`.
"""

from __future__ import annotations

import json
import logging
import pathlib
import platform
from dataclasses import dataclass
from functools import partial
from typing import Callable, Literal

import numpy as np
import pytest

from tabpfn.preprocessing.configs import EnsembleConfig, PreprocessorConfig
from tabpfn.preprocessing.datamodel import Feature
from tabpfn.preprocessing.pipeline_factory import create_preprocessing_pipeline

try:
    from tabpfn.preprocessing.datamodel import FeatureModality, FeatureSchema

    NEW_PIPELINE_IMPLEMENTATION = True
except ImportError:
    try:
        from tabpfn.preprocessing.pipeline_interface import TransformResult
    except ImportError:
        pytest.skip("Pipeline consistency tests cannot be run", allow_module_level=True)
    NEW_PIPELINE_IMPLEMENTATION = False

logger = logging.getLogger(__name__)

RANDOM_STATE = 42
ENABLED_PLATFORMS = ["darwin_arm64"]
"""The tests are enabled if _get_current_platform() returns a string in this set.
For now we just test on MacOS, and simply identifying that seems to be enough
to get consistent outputs.
"""
N_SAMPLES = 50
N_TEST_SAMPLES = 5


def _get_random_data_with_categoricals(
    random_state: np.random.Generator,
    n_samples: int = 20,
    n_numerical: int = 3,
    n_categorical: int = 2,
) -> tuple[np.ndarray, FeatureSchema | list[int]]:
    """Generate random data with both numerical and categorical features."""
    n_features = n_numerical + n_categorical
    X = np.zeros((n_samples, n_features), dtype=np.float64)

    # Numerical features: random continuous values with some variation
    X[:, :n_numerical] = random_state.standard_normal((n_samples, n_numerical)) * 10

    # Add some NaNs to numerical features to test NaN handling
    nan_mask = random_state.random((n_samples, n_numerical)) < 0.1
    X[:, :n_numerical][nan_mask] = np.nan

    # Categorical features: random integers representing categories
    for i in range(n_numerical, n_features):
        num_categories = random_state.integers(2, 6)
        X[:, i] = random_state.integers(0, num_categories, size=n_samples).astype(float)

    if NEW_PIPELINE_IMPLEMENTATION:
        # Build column metadata
        features = [
            Feature(name=None, modality=FeatureModality.NUMERICAL)  # type: ignore
            for _ in range(n_numerical)
        ] + [
            Feature(name=None, modality=FeatureModality.CATEGORICAL)  # type: ignore
            for _ in range(n_categorical)
        ]
        schema = FeatureSchema(features=features)  # type: ignore
        return X, schema
    categorical_indices = list(range(n_numerical, n_features))
    return X, categorical_indices


def _create_ensemble_config(
    preprocess_config: PreprocessorConfig,
    *,
    add_fingerprint_feature: bool = False,
    polynomial_features: Literal["no", "all"] | int = "no",
    feature_shift_count: int = 0,
    feature_shift_decoder: Literal["shuffle", "rotate"] | None = None,
) -> EnsembleConfig:
    """Create an EnsembleConfig for testing."""
    return EnsembleConfig(
        preprocess_config=preprocess_config,
        add_fingerprint_feature=add_fingerprint_feature,
        polynomial_features=polynomial_features,
        feature_shift_count=feature_shift_count,
        feature_shift_decoder=feature_shift_decoder,
        subsample_ix=None,
        outlier_removal_std=None,
        _model_index=0,
    )


@dataclass(frozen=True)
class _PipelineConsistencyCase:
    """Test case for pipeline consistency testing."""

    config_factory: Callable[[], EnsembleConfig]
    description: str


# Define test cases covering various configuration options
_PREPROCESSOR_CONFIGS: dict[str, PreprocessorConfig] = {
    "none_numeric": PreprocessorConfig(
        name="none",
        categorical_name="numeric",
    ),
    "none_ordinal": PreprocessorConfig(
        name="none",
        categorical_name="ordinal",
    ),
    "none_onehot": PreprocessorConfig(
        name="none",
        categorical_name="onehot",
    ),
    "none_ordinal_shuffled": PreprocessorConfig(
        name="none",
        categorical_name="ordinal_shuffled",
    ),
    "quantile_uni_coarse": PreprocessorConfig(
        name="quantile_uni_coarse",
        categorical_name="numeric",
    ),
    "squashing_scaler": PreprocessorConfig(
        name="squashing_scaler_default",
        categorical_name="ordinal_very_common_categories_shuffled",
    ),
    "robust_onehot": PreprocessorConfig(
        name="robust",
        categorical_name="onehot",
    ),
    "quantile_append_original": PreprocessorConfig(
        name="quantile_uni_coarse",
        categorical_name="numeric",
        append_original=True,
    ),
    "none_with_svd": PreprocessorConfig(
        name="none",
        categorical_name="numeric",
        global_transformer_name="svd",
    ),
    "squashing_with_svd_quarter": PreprocessorConfig(
        name="squashing_scaler_default",
        categorical_name="ordinal_very_common_categories_shuffled",
        global_transformer_name="svd_quarter_components",
    ),
    # TODO, add test for subsampling via max_features_per_estimator setting
}


def _build_test_cases() -> dict[str, _PipelineConsistencyCase]:
    """Build all test cases from preprocessor configs and variations."""
    test_cases: dict[str, _PipelineConsistencyCase] = {}

    # Basic preprocessor config tests
    for config_name, preprocess_config in _PREPROCESSOR_CONFIGS.items():
        test_cases[f"basic_{config_name}"] = _PipelineConsistencyCase(
            config_factory=partial(_create_ensemble_config, preprocess_config),
            description=f"Basic pipeline with {config_name}",
        )

    # Test with fingerprint features
    for config_name in ["none_numeric", "quantile_uni_coarse", "squashing_scaler"]:
        preprocess_config = _PREPROCESSOR_CONFIGS[config_name]
        test_cases[f"fingerprint_{config_name}"] = _PipelineConsistencyCase(
            config_factory=partial(
                _create_ensemble_config, preprocess_config, add_fingerprint_feature=True
            ),
            description=f"Pipeline with fingerprint and {config_name}",
        )

    # Test with polynomial features
    for config_name in ["none_numeric", "quantile_uni_coarse"]:
        preprocess_config = _PREPROCESSOR_CONFIGS[config_name]
        test_cases[f"poly_all_{config_name}"] = _PipelineConsistencyCase(
            config_factory=partial(
                _create_ensemble_config, preprocess_config, polynomial_features="all"
            ),
            description=f"Pipeline with poly=all and {config_name}",
        )
        test_cases[f"poly_3_{config_name}"] = _PipelineConsistencyCase(
            config_factory=partial(
                _create_ensemble_config, preprocess_config, polynomial_features=3
            ),
            description=f"Pipeline with poly=3 and {config_name}",
        )

    # Test with feature shuffling
    for config_name in ["none_numeric", "squashing_scaler"]:
        preprocess_config = _PREPROCESSOR_CONFIGS[config_name]
        test_cases[f"shuffle_{config_name}"] = _PipelineConsistencyCase(
            config_factory=partial(
                _create_ensemble_config,
                preprocess_config,
                feature_shift_count=3,
                feature_shift_decoder="shuffle",
            ),
            description=f"Pipeline with shuffle and {config_name}",
        )
        test_cases[f"rotate_{config_name}"] = _PipelineConsistencyCase(
            config_factory=partial(
                _create_ensemble_config,
                preprocess_config,
                feature_shift_count=2,
                feature_shift_decoder="rotate",
            ),
            description=f"Pipeline with rotate and {config_name}",
        )

    # Combined test: fingerprint + polynomial + shuffle
    test_cases["combined_all_options"] = _PipelineConsistencyCase(
        config_factory=partial(
            _create_ensemble_config,
            _PREPROCESSOR_CONFIGS["quantile_uni_coarse"],
            add_fingerprint_feature=True,
            polynomial_features=3,
            feature_shift_count=2,
            feature_shift_decoder="shuffle",
        ),
        description="Pipeline with all options enabled",
    )

    return test_cases


test_cases = _build_test_cases()


def _get_current_platform_string() -> str:
    """Return a string identifying the current platform."""
    if platform.system() == "Darwin" and platform.machine() == "arm64":
        return "darwin_arm64"
    return "unknown"


HOW_TO_FIX_MESSAGE = (
    "If this is expected, regenerate the reference data by running: "
    "python tests/test_preprocessing/test_pipeline_consistency.py"
)
REFERENCE_DIR = (
    pathlib.Path(__file__).parent / "reference_preprocessing" / "pipeline_consistency"
)
"""The directory that contains the reference preprocessing outputs."""


def _get_platform_dir() -> pathlib.Path:
    """Return the platform-specific directory for reference data."""
    return REFERENCE_DIR / _get_current_platform_string()


def _get_reference_path(test_name: str) -> pathlib.Path:
    """Get the path to the reference file for a test case."""
    return _get_platform_dir() / f"{test_name}.json"


def _transform_with_pipeline(
    test_case: _PipelineConsistencyCase,
) -> tuple[np.ndarray, np.ndarray, dict[str, list[int]]]:
    """Run the preprocessing pipeline and return transformed data."""
    rng = np.random.default_rng(RANDOM_STATE)
    X_train, schema_or_cat_indices = _get_random_data_with_categoricals(
        rng, n_samples=N_SAMPLES
    )
    X_test, _ = _get_random_data_with_categoricals(rng, n_samples=N_TEST_SAMPLES)

    config = test_case.config_factory()
    pipeline = create_preprocessing_pipeline(config, random_state=RANDOM_STATE)

    result_train = pipeline.fit_transform(X_train, schema_or_cat_indices)
    result_test = pipeline.transform(X_test)

    if NEW_PIPELINE_IMPLEMENTATION:
        schema_dict: dict[str, list[int]] = {"categorical": []}
        for idx, feature in enumerate(result_train.feature_schema.features):
            key = feature.modality.value
            if key not in schema_dict:
                schema_dict[key] = []
            schema_dict[key].append(idx)
    else:
        assert isinstance(result_train, TransformResult)
        _, categorical_indices = result_train
        schema_dict = {
            "categorical": categorical_indices,
        }

    # Ensure we return numpy arrays (pipeline may return tensors in some cases)
    X_train_out = np.asarray(result_train.X)
    X_test_out = np.asarray(result_test.X)

    return X_train_out, X_test_out, schema_dict


def _load_json_to_array(data: list) -> np.ndarray:
    """Convert a JSON list back to numpy array, restoring NaN values."""
    return np.array(data, dtype=np.float64)


def _load_reference_or_fail(
    test_name: str,
) -> tuple[np.ndarray, np.ndarray, dict[str, list[int]]]:
    """Load reference data from file or fail with helpful message.

    Args:
        test_name: Name of the test case.

    Returns:
        Tuple of (X_train_ref, X_test_ref, metadata_dict).
    """
    path = _get_reference_path(test_name)
    if not path.exists():
        raise AssertionError(f"Reference data missing at {path}\n{HOW_TO_FIX_MESSAGE}")

    with path.open("r") as f:
        data = json.load(f)

    return (
        _load_json_to_array(data["X_train"]),
        _load_json_to_array(data["X_test"]),
        data["metadata"],
    )


def _save_reference(
    test_name: str,
    X_train: np.ndarray,
    X_test: np.ndarray,
    metadata: dict[str, list[int]],
) -> None:
    """Save reference data to file."""
    path = _get_reference_path(test_name)
    path.parent.mkdir(parents=True, exist_ok=True)

    # Replace NaN with null for JSON serialization
    X_train_list = np.where(np.isnan(X_train), None, X_train).tolist()
    X_test_list = np.where(np.isnan(X_test), None, X_test).tolist()

    with path.open("w") as f:
        json.dump(
            {
                "X_train": X_train_list,
                "X_test": X_test_list,
                "metadata": metadata,
            },
            f,
            indent=2,
        )

    logger.info(f"Reference data saved for {test_name} at {path}")


@pytest.mark.skipif(
    _get_current_platform_string() not in ENABLED_PLATFORMS,
    reason="Current platform does not have consistency tests enabled.",
)
@pytest.mark.parametrize(("test_case_name", "test_case"), test_cases.items())
def test__pipeline__output_matches_reference(
    test_case_name: str, test_case: _PipelineConsistencyCase
) -> None:
    """Test that pipeline output matches saved reference data."""
    X_train_ref, X_test_ref, metadata_ref = _load_reference_or_fail(test_case_name)
    X_train, X_test, schema = _transform_with_pipeline(test_case)

    assert X_train.shape == X_train_ref.shape, (
        f"Training data shape mismatch: {X_train.shape} vs {X_train_ref.shape}\n"
        f"{HOW_TO_FIX_MESSAGE}"
    )
    assert X_test.shape == X_test_ref.shape, (
        f"Test data shape mismatch: {X_test.shape} vs {X_test_ref.shape}\n"
        f"{HOW_TO_FIX_MESSAGE}"
    )

    assert schema["categorical"] == metadata_ref["categorical"], (
        f"Metadata mismatch:\nGot: {schema['categorical']}\nExpected: "
        f"{metadata_ref['categorical']}\n"
        f"{HOW_TO_FIX_MESSAGE}"
    )

    np.testing.assert_allclose(
        X_train,
        X_train_ref,
        rtol=1e-6,
        atol=1e-6,
        equal_nan=True,
        err_msg=f"Training data values changed.\n{HOW_TO_FIX_MESSAGE}",
    )
    np.testing.assert_allclose(
        X_test,
        X_test_ref,
        rtol=1e-6,
        atol=1e-6,
        equal_nan=True,
        err_msg=f"Test data values changed.\n{HOW_TO_FIX_MESSAGE}",
    )


def save_reference_data() -> None:
    """Generate and save reference data for all test cases."""
    _get_platform_dir().mkdir(parents=True, exist_ok=True)

    for test_case_name, test_case in test_cases.items():
        try:
            X_train, X_test, schema = _transform_with_pipeline(test_case)
            _save_reference(test_case_name, X_train, X_test, schema)
        except Exception as e:
            logger.error(f"Failed to generate reference for {test_case_name}: {e}")
            raise


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    save_reference_data()
